SVM

Best Parameters
C	0.5
degree	1
kernel	rbf
max_iter	2000
random_state	1
tol	0.0005

Confusion Matrix
127	0	
2	142	

Model Result
              precision    recall  f1-score   support

         0.0       0.98      1.00      0.99       127
         1.0       1.00      0.99      0.99       144

   micro avg       0.99      0.99      0.99       271
   macro avg       0.99      0.99      0.99       271
weighted avg       0.99      0.99      0.99       271



Decision Tree

Best Parameters
criterion	gini
random_state	1

Confusion Matrix
121	6	
4	140	

Model Result
              precision    recall  f1-score   support

         0.0       0.97      0.95      0.96       127
         1.0       0.96      0.97      0.97       144

   micro avg       0.96      0.96      0.96       271
   macro avg       0.96      0.96      0.96       271
weighted avg       0.96      0.96      0.96       271



KNN

Best Parameters
algorithm	ball_tree
n_neighbors	2

Confusion Matrix
127	0	
1	143	

Model Result
              precision    recall  f1-score   support

         0.0       0.99      1.00      1.00       127
         1.0       1.00      0.99      1.00       144

   micro avg       1.00      1.00      1.00       271
   macro avg       1.00      1.00      1.00       271
weighted avg       1.00      1.00      1.00       271



Logistic Regression

Best Parameters
C	0.5
max_iter	400
multi_class	multinomial
random_state	1
solver	newton-cg
tol	0.0005

Confusion Matrix
125	2	
2	142	

Model Result
              precision    recall  f1-score   support

         0.0       0.98      0.98      0.98       127
         1.0       0.99      0.99      0.99       144

   micro avg       0.99      0.99      0.99       271
   macro avg       0.99      0.99      0.99       271
weighted avg       0.99      0.99      0.99       271



Random Forest

Best Parameters
criterion	entropy
n_estimators	100
random_state	1

Confusion Matrix
127	0	
2	142	

Model Result
              precision    recall  f1-score   support

         0.0       0.98      1.00      0.99       127
         1.0       1.00      0.99      0.99       144

   micro avg       0.99      0.99      0.99       271
   macro avg       0.99      0.99      0.99       271
weighted avg       0.99      0.99      0.99       271



AdaBoost

Best Parameters
base_estimator	DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=1,
            splitter='best')
learning_rate	0.8
random_state	1

Confusion Matrix
127	0	
2	142	

Model Result
              precision    recall  f1-score   support

         0.0       0.98      1.00      0.99       127
         1.0       1.00      0.99      0.99       144

   micro avg       0.99      0.99      0.99       271
   macro avg       0.99      0.99      0.99       271
weighted avg       0.99      0.99      0.99       271



XGBoost

Best Parameters
booster	gbtree
learning_rate	0.5
n_estimators	50
objective	binary:logitraw
random_state	1
subsample	0.8

Confusion Matrix
127	0	
3	141	

Model Result
              precision    recall  f1-score   support

         0.0       0.98      1.00      0.99       127
         1.0       1.00      0.98      0.99       144

   micro avg       0.99      0.99      0.99       271
   macro avg       0.99      0.99      0.99       271
weighted avg       0.99      0.99      0.99       271



